{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-*- coding: utf-8 -*-\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from collections import Counter\n",
    "import ast\n",
    "from konlpy.tag import Hannanum\n",
    "# from konlpy.tag import Kkma\n",
    "# from konlpy.tag import Komoran\n",
    "# from konlpy.tag import Mecab\n",
    "# from konlpy.tag import Twitter\n",
    "from konlpy.utils import concordance, pprint\n",
    "import datetime\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import sys\n",
    "reload(sys)\n",
    "import psutil\n",
    "sys.setdefaultencoding('utf-8')\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tags(tags):\n",
    "    dic_tag = {}\n",
    "    for el in tags:\n",
    "        tag = el[1]\n",
    "        if not tag in dic_tag:\n",
    "            dic_tag[tag] = 1\n",
    "        else:\n",
    "            dic_tag[tag] += 1\n",
    "    return dic_tag\n",
    "\n",
    "#수식언, 체언, 용언만 딕셔너리로 리턴\n",
    "#Pos == PartOfSpeech\n",
    "def filter_pos(pos_of_article, tagger):\n",
    "    if tagger == 'han':\n",
    "        # 수식언, 체언, 용언\n",
    "        filtered_pos = {'M':{}, 'N':{}, 'P':{}}\n",
    "        permitted_pos = ['M','N','P']\n",
    "#     elif tagger in ['komo', 'kkma']:\n",
    "#         # 고유명사, 보통명사, 대명사, 수사, 형용사, 동사, 어근\n",
    "#         dic_tag = {'NNG':{}, 'NNP':{},'NP':{},'NR':{},'VA':{},'VV':{},'XR':{}}\n",
    "#         arr_tag = ['NNG','NNP','NP','NR','VA','VV','XR']\n",
    "    for el in pos_of_article:\n",
    "        word = el[0]\n",
    "        pos = el[1]\n",
    "        if pos in permitted_pos:\n",
    "            if word in filtered_pos[pos]:\n",
    "                filtered_pos[pos][word] += 1\n",
    "            else:\n",
    "                filtered_pos[pos][word] = 1\n",
    "    return filtered_pos\n",
    "\n",
    "def processing_words(dic):\n",
    "    processed_hannanum = {}\n",
    "    for key in dic.keys():\n",
    "        x = sorted(dic[key].items(), key=itemgetter(1), reverse = True)\n",
    "        processed_hannanum[key] = x\n",
    "    return processed_hannanum\n",
    "\n",
    "def print_result(result):\n",
    "    if type(result) == list:\n",
    "        for item in result:\n",
    "            pprint(item)\n",
    "    else:\n",
    "        for key, value in result.items():\n",
    "            pprint(key)\n",
    "            pprint(value)\n",
    "\n",
    "def get_total_words(dic):\n",
    "    total = 0\n",
    "    for tag in dic.keys():\n",
    "        dic_specific_tag = dic[tag]\n",
    "        total += sum(dic_specific_tag.values())\n",
    "    return total\n",
    "\n",
    "def get_unique_words(dic):\n",
    "    total = 0\n",
    "    for tag in dic.keys():\n",
    "        dic_specific_tag = dic[tag]\n",
    "        total += len(dic_specific_tag)\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_score(dic1, dic2, total_dic, algorithm):\n",
    "    same_words = 0\n",
    "    total_words = 0\n",
    "    similarity_score = 0\n",
    "    if algorithm == 1:\n",
    "        #same words between dic1 and dic2 / total_words of dic1 and dic2\n",
    "        for tag in dic1.keys(): #['M', 'N', 'P']\n",
    "            tagged_words_from_dic1 = dic1[tag]\n",
    "            tagged_words_from_dic2 = dic2[tag]\n",
    "            for word in tagged_words_from_dic1.keys():\n",
    "                if word in tagged_words_from_dic2:\n",
    "                    word_count = tagged_words_from_dic1[word] + tagged_words_from_dic2[word]\n",
    "                    same_words += word_count\n",
    "        total_words = get_total_words(dic1) + get_total_words(dic2)\n",
    "        similarity_score = float(same_words) / float(total_words)\n",
    "\n",
    "        print('similarity score:', similarity_score)\n",
    "        return round(similarity_score, 3)\n",
    "\n",
    "    elif algorithm == 2:\n",
    "        #unique same words between dic1 and dic2 / unique total words of total dic(dic1+dic2)\n",
    "        for tag in dic1.keys():\n",
    "            dic1_specific_tag = dic1[tag]\n",
    "            dic2_specific_tag = dic2[tag]\n",
    "            for word in dic1_specific_tag.keys():\n",
    "                if word in dic2_specific_tag:\n",
    "                    same_words += 1\n",
    "        total_words = get_unique_words(total_dic)\n",
    "        similarity_score = float(same_words) / float(total_words)\n",
    "\n",
    "        print('similarity score:', similarity_score)\n",
    "        return round(similarity_score, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class calculate_similarity_of():\n",
    "\n",
    "    def __init__(self, df):\n",
    "        df = df.dropna(subset=['Content']).reset_index(drop=True)\n",
    "        df = df.sort_values('Date').reset_index(drop=True)\n",
    "        df['pos_tagged_words'] = None\n",
    "        self.df = df\n",
    "\n",
    "    def _get_pos_tagged_words_(self, article, index):\n",
    "        if index == None:\n",
    "            pos_tagged_words = None\n",
    "            content = article\n",
    "        else:\n",
    "            pos_tagged_words = self.df.pos_tagged_words[index]\n",
    "            content = article.Content\n",
    "\n",
    "        if pos_tagged_words == None:\n",
    "            #print(\"Update for acticle\", index, \"started\")\n",
    "            pos_article = Hannanum().pos(content.replace(' \\n', '').replace('\\n ',''))\n",
    "            #print('Pos is tagged for article')\n",
    "            pos_tagged_words = filter_pos(pos_article, 'han')\n",
    "            #print('Pos is filtered')\n",
    "            if index != None:\n",
    "                self.df.loc[index, 'pos_tagged_words'] = repr(pos_tagged_words)\n",
    "                #print('pos_tagged_words_saved')\n",
    "                print(\"Get article\", index, \"'s pos_tagged_words\")\n",
    "        else:\n",
    "            pos_tagged_words = ast.literal_eval(pos_tagged_words)\n",
    "            print(\"Get article\", index, \"'s pos_tagged_words\")\n",
    "        return pos_tagged_words\n",
    "    \n",
    "    \n",
    "    def _find_articles_to_compare_(self, i):\n",
    "        needed_article_number = 10\n",
    "        index_list = []\n",
    "        finding_articles = True\n",
    "        yesterday = self.df.Date[i] - datetime.timedelta(1)\n",
    "        firstday = self.df.Date[0]\n",
    "        while finding_articles:\n",
    "            yesterday_articles = self.df[self.df.Date == yesterday]\n",
    "            if len(yesterday_articles) == 0:\n",
    "                yesterday -= datetime.timedelta(1)\n",
    "                #print('Keep going back to', yesterday)\n",
    "                if yesterday < firstday:\n",
    "                    yesterday_articles = self.df.loc[i-needed_article_number:i-1]\n",
    "                    index_list += list(yesterday_articles.index)\n",
    "                    finding_articles = False\n",
    "                continue\n",
    "            elif len(yesterday_articles) >= needed_article_number:\n",
    "                index_list += random.sample(yesterday_articles.index, needed_article_number)\n",
    "                finding_articles = False\n",
    "                #print('Found all articles to compare')\n",
    "            else:\n",
    "                index_list += list(yesterday_articles.index)\n",
    "                needed_article_number -= len(yesterday_articles)\n",
    "                yesterday -= datetime.timedelta(1)\n",
    "                #print('Found', len(yesterday_articles), 'articles and need more')\n",
    "        return self.df.iloc[index_list]\n",
    "    \n",
    "    \n",
    "    def on(self, from_number, to_number):\n",
    "        process = psutil.Process(os.getpid())\n",
    "        article_df = self.df\n",
    "        start = time.time()\n",
    "        for i in range(from_number, to_number):\n",
    "            print('from', from_number, 'to', to_number)\n",
    "            start_i = time.time()\n",
    "            current_article = article_df.iloc[i]\n",
    "            compared_articles = self._find_articles_to_compare_(i)\n",
    "            j = 1\n",
    "            for ci, compared_article in compared_articles.iterrows():\n",
    "                start_j = time.time()\n",
    "                total_article_content = current_article.Content + compared_article.Content\n",
    "\n",
    "                current_article_words = self._get_pos_tagged_words_(current_article, i)\n",
    "                compared_article_words = self._get_pos_tagged_words_(compared_article, ci)\n",
    "                total_article_words = self._get_pos_tagged_words_(total_article_content, None)\n",
    "\n",
    "                similarity_score1 = calculate_score(current_article_words, compared_article_words, total_article_words, 1)\n",
    "                similarity_score2 = calculate_score(current_article_words, compared_article_words, total_article_words, 2)\n",
    " \n",
    "                article_df.loc[i, 'si_1_{j}'.format(j=j)] = similarity_score1\n",
    "                article_df.loc[i, 'si_2_{j}'.format(j=j)] = similarity_score2\n",
    "                j += 1\n",
    "                end_j = time.time()\n",
    "                print('comparing one article takes', end_j - start_j, 'seconds', 'now:', str(datetime.datetime.today()))\n",
    "                print('total_cpu:', psutil.cpu_percent(percpu=True),\\\n",
    "                      'process_cpu:', process.cpu_percent())\n",
    "\n",
    "                print('p_using_memory:',process.memory_full_info().rss/1024/1024, \\\n",
    "                      'p_using_v_memory:', process.memory_full_info().vms/1024/1024)\n",
    "\n",
    "                print('total_memory:', psutil.virtual_memory().total/1024/1024,\\\n",
    "                      'available_memory:', psutil.virtual_memory().available/1024/1024,\n",
    "                      'using_percent:', psutil.virtual_memory().percent)\n",
    "            average1 = article_df.loc[i, ['si_1_{j}'.format(j=j) for j in range(1,11)]].mean()\n",
    "            average2 = article_df.loc[i, ['si_2_{j}'.format(j=j) for j in range(1,11)]].mean()\n",
    "            article_df.loc[i, 'average1'] = average1\n",
    "            article_df.loc[i, 'average2'] = average2\n",
    "            end_i = time.time()\n",
    "            print('comparing 10 articles takes', end_i-start_i, 'seconds')\n",
    "        end = time.time()\n",
    "        print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = '/home/ubuntu/ubuntu/'\n",
    "files = os.listdir(folder)\n",
    "\n",
    "files = filter(lambda x: 'xlsx' in x , files)\n",
    "\n",
    "number = re.compile('\\d{3,5}(?=.xlsx)')\n",
    "\n",
    "number_and_files = []\n",
    "for f in files:\n",
    "    try:\n",
    "        count = int(number.findall(f)[0])\n",
    "        number_and_files.append([count, f])\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "file_addr_sorted = sorted(number_and_files, key = itemgetter(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = file_addr_sorted[8:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(file_addr_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fi in files:\n",
    "    file_address = folder + fi[1]\n",
    "    address_for_save = folder + 'complete/' + fi[1][:-5] + '_completed.xlsx'\n",
    "    print('read', file_address, 'will be saved in', address_for_save)\n",
    "    article_df = pd.read_excel(file_address)\n",
    "    call = calculate_similarity_of(article_df)\n",
    "    call.on(10, len(call.df))\n",
    "    call.df.to_excel(address_for_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
